{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce66e424",
   "metadata": {},
   "source": [
    "# Bank Marketing Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e41204",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This notebook demonstrates a complete machine learning workflow for a tabular classification task. We will use the Bank Marketing dataset from the UCI Machine Learning Repository to predict whether a customer will subscribe to a term deposit.\n",
    "\n",
    "The workflow covers the following steps:\n",
    "1.  **Data Loading and Initial Exploration**: Loading the data and getting a first look at its structure.\n",
    "2.  **Data Preprocessing and Cleaning**: Renaming columns, correcting data types, and handling missing values.\n",
    "3.  **Exploratory Data Analysis (EDA)**: Visualizing the data to understand feature distributions and relationships.\n",
    "4.  **Feature Engineering**: Preparing the data for modeling by scaling and encoding features.\n",
    "5.  **Model Training**: Training multiple classification models.\n",
    "6.  **Model Evaluation**: Evaluating model performance using various metrics.\n",
    "7.  **Prediction**: Using the trained models to make predictions on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97046316",
   "metadata": {},
   "source": [
    "## 2. Setup: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7028bb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix, \n",
    "                                roc_auc_score, roc_curve,auc)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder,LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1054b7de",
   "metadata": {},
   "source": [
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b66915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "notebook_path = Path.cwd()\n",
    "file_path = notebook_path.parent / 'dataset' / 'bank.csv'\n",
    "print(f\"Loading data from: {file_path}\")\n",
    "data = pd.read_csv(file_path, header=0, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4506aa84",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784ffc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_n_change(data):\n",
    "    # Rename columns for better readability\n",
    "    data.rename(columns={\n",
    "        'marital':'marital_status',\n",
    "        'default':'credit_default',\n",
    "        'housing':'housing_loan',\n",
    "        'loan':'personal_loan',\n",
    "        'y':'response'}, inplace=True)\n",
    "    #change data types for faster loading\n",
    "    data['response'] = data['response'].astype('category')\n",
    "    data['marital_status'] = data['marital_status'].astype('category')\n",
    "    data['education'] = data['education'].astype('category')\n",
    "    data['job'] = data['job'].astype('category')\n",
    "    data['contact'] = data['contact'].astype('category')\n",
    "    data['month'] = data['month'].astype('category')\n",
    "    data['day'] = data['day'].astype('category')\n",
    "    data['credit_default'] = data['credit_default'].astype('category')\n",
    "    data['housing_loan'] = data['housing_loan'].astype('category')\n",
    "    data['personal_loan'] = data['personal_loan'].astype('category')\n",
    "    return data\n",
    "data=rename_n_change(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc02236",
   "metadata": {},
   "source": [
    "### 4.1. Handle Missing Values\n",
    "\n",
    "We check the `poutcome` column for missing values and drop it since it contains over 80% unknown values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc636ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['poutcome'].value_counts(dropna=False)/len(data)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cb62d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop poutcome with more than 80% unknown values\n",
    "data.drop('poutcome', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72007c25",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4c0d79",
   "metadata": {},
   "source": [
    "### 5.1. Check for Data Imbalance\n",
    "\n",
    "We visualize the distribution of the target variable `response` to check for class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a217ad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['response'].value_counts().plot(kind='bar', color=[\"#F00E0E\", \"#339708\"])\n",
    "plt.title('Distribution of Response Variable')\n",
    "plt.xlabel('Response')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb43ac6a",
   "metadata": {},
   "source": [
    "### 5.2. Visualize Feature Distributions\n",
    "\n",
    "We use Seaborn to create various plots to understand the distribution of individual features and their relationships with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c9abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "data['housing_loan'].value_counts().plot(kind='bar',color=[\"#0EF0C7\", \"#970808\"])\n",
    "plt.title('Distribution of Housing Loan')\n",
    "plt.xlabel('Housing Loan')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4264fb1",
   "metadata": {},
   "source": [
    "### 5.3. Correlation Matrix\n",
    "\n",
    "We create a correlation matrix to understand the relationships between numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7752dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation matrix for numeric features\n",
    "plt.figure(figsize=(12, 8))\n",
    "numeric_ft = data[['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']]\n",
    "corr_matrix = numeric_ft.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3210dfc",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering and Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f74fe3",
   "metadata": {},
   "source": [
    "### 6.1. Assign Features and Labels\n",
    "\n",
    "We separate the dataset into features (X) and the target variable (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aa3854",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.drop(columns=['response'])\n",
    "y=data['response']\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152ae0cf",
   "metadata": {},
   "source": [
    "### 6.2. Split Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b00590",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training and testing sets\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.8,stratify=y,random_state=78)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5403dcdb",
   "metadata": {},
   "source": [
    "## 7. Data Transformation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed07d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate numeric and categorical features\n",
    "numeric_features = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
    "categorical_features = ['job', 'marital_status', 'education', 'month', 'housing_loan', 'personal_loan', 'contact', 'credit_default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4498783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column transformer to apply different preprocessing steps to different columns\n",
    "pre_processor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "# Create a pipeline that first applies the preprocessor and then fits a classifier\n",
    "pre_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', pre_processor)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e7f19b",
   "metadata": {},
   "source": [
    "### 7.1. Label Encode Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de054a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f4619d",
   "metadata": {},
   "source": [
    "### 7.2. Apply Preprocessing to Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef81513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit preprocessing on training data and transform both sets\n",
    "X_train = pre_pipeline.fit_transform(X_train)\n",
    "X_test = pre_pipeline.transform(X_test)  # No fitting on test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11e6594",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fdc894",
   "metadata": {},
   "source": [
    "### 7.3. Handle Class Imbalance with SMOTE\n",
    "\n",
    "We use Synthetic Minority Over-sampling Technique (SMOTE) to address the class imbalance in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30292ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE()\n",
    "X_train,y_train = smote.fit_resample(X_train, y_train) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e962368c",
   "metadata": {},
   "source": [
    "## 8. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ac5080",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate and train\n",
    "logreg=LogisticRegression(class_weight= 'balanced')\n",
    "dtree=DecisionTreeClassifier()\n",
    "rforest=RandomForestClassifier(class_weight= 'balanced',n_estimators=100)\n",
    "gbm=GradientBoostingClassifier()\n",
    "gnb=GaussianNB()\n",
    "knn=KNeighborsClassifier()\n",
    "xgb = XGBClassifier()\n",
    "lgbm = LGBMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c85fdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict, Evaluate and plot\n",
    "models = {\"XGBoost\": xgb, \"LightGBM\": lgbm,\"Random Forest\": rforest, \"Gradient Boosting\": gbm,\"Logistic Regression\": logreg,\"K Neighbors\": knn,\"Gaussian Naive Bayes\": gnb,\"Decision Tree\": dtree}\n",
    "results = pd.DataFrame(columns=['Model', 'Accuracy', 'ROC_AUC_Score'])\n",
    "plt.figure(figsize=(10, 8))\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    new_row = pd.DataFrame([{'Model': name, 'Accuracy': accuracy, 'ROC_AUC_Score': roc_auc}])\n",
    "    results = pd.concat([results, new_row], ignore_index=True)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79be2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the results by ROC_AUC_Score in descending order\n",
    "results = results.sort_values(by='ROC_AUC_Score', ascending = False,ignore_index = True)\n",
    "# Display the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b538f",
   "metadata": {},
   "source": [
    "## 9. Model Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee62209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "joblib.dump(logreg, '../saved_models/logreg_model.pkl')\n",
    "joblib.dump(dtree, '../saved_models/dtree_model.pkl')\n",
    "joblib.dump(rforest, '../saved_models/rforest_model.pkl')\n",
    "joblib.dump(gbm, '../saved_models/gbm_model.pkl')\n",
    "joblib.dump(gnb, '../saved_models/gnb_model.pkl')\n",
    "joblib.dump(knn, '../saved_models/knn_model.pkl')\n",
    "joblib.dump(xgb, '../saved_models/xgb_model.pkl')\n",
    "joblib.dump(lgbm, '../saved_models/lgbm_model.pkl')\n",
    "joblib.dump(pre_pipeline, '../saved_models/pre_pipeline.pkl')\n",
    "joblib.dump(label_encoder, '../saved_models/label_encoder.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4723b0a2",
   "metadata": {},
   "source": [
    "## 10. Prediction on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380628b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "logreg_ = joblib.load('../saved_models/logreg_model.pkl')\n",
    "dtree_ = joblib.load('../saved_models/dtree_model.pkl')\n",
    "rforest_ = joblib.load('../saved_models/rforest_model.pkl')\n",
    "gbm_ = joblib.load('../saved_models/gbm_model.pkl')\n",
    "gnb_ = joblib.load('../saved_models/gnb_model.pkl')\n",
    "knn_ = joblib.load('../saved_models/knn_model.pkl')\n",
    "xgb_ = joblib.load('../saved_models/xgb_model.pkl')\n",
    "lgbm_ = joblib.load('../saved_models/lgbm_model.pkl')\n",
    "pre_pipeline_ = joblib.load('../saved_models/pre_pipeline.pkl')\n",
    "label_encoder_ = joblib.load('../saved_models/label_encoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3e7b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on new data\n",
    "new_data = pd.DataFrame({\n",
    "    'age': [30],\n",
    "    'balance': [1000],\n",
    "    'day': [15],\n",
    "    'duration': [200],\n",
    "    'campaign': [1],\n",
    "    'pdays': [999],\n",
    "    'previous': [0],\n",
    "    'job': ['admin.'],\n",
    "    'contact': ['cellular'],\n",
    "    'marital_status': ['single'],\n",
    "    'education': ['university.degree'],\n",
    "    'month': ['may'],\n",
    "    'housing_loan': ['yes'],\n",
    "    'personal_loan': ['no'],\n",
    "    'credit_default': ['no']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5ad69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the new data\n",
    "new_d = pd.DataFrame(pre_pipeline_.transform(new_data), columns=pre_pipeline_.get_feature_names_out()) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35d8ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions\n",
    "logreg_pred = logreg_.predict(new_d)\n",
    "dtree_pred = dtree_.predict(new_d)\n",
    "rforest_pred = rforest_.predict(new_d)\n",
    "gbm_pred = gbm_.predict(new_d)\n",
    "gnb_pred = gnb_.predict(new_d)\n",
    "knn_pred = knn_.predict(new_d)\n",
    "xgb_pred = xgb_.predict(new_d)\n",
    "lgbm_pred = lgbm_.predict(new_d)\n",
    "# Print the predictions\n",
    "print(\"XGBoost Prediction: \", xgb_pred)\n",
    "print(\"LightGBM Prediction: \", lgbm_pred)\n",
    "print(\"Random Forest Prediction: \", rforest_pred)\n",
    "print(\"Gradient Boosting Prediction: \", gbm_pred)\n",
    "print(\"Gaussian Naive Bayes Prediction: \", gnb_pred)\n",
    "print(\"Logistic Regression Prediction: \", logreg_pred)\n",
    "print(\"Decision Tree Prediction: \", dtree_pred)\n",
    "print(\"K Neighbors Prediction: \", knn_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
